# Decisiones Arquitect√≥nicas

**√öltima actualizaci√≥n**: 2025-11-22
**Prop√≥sito**: Documentar las decisiones t√©cnicas clave con su justificaci√≥n completa

---

## üìê Principios de Dise√±o

### 1. Conversaci√≥n como "Llamada Telef√≥nica"

**Decisi√≥n**: La conversaci√≥n debe sentirse como una llamada telef√≥nica en tiempo real, no como un chat con delays.

**Implicaciones**:
- ‚úÖ Audio streaming (no batch processing)
- ‚úÖ Detecci√≥n autom√°tica de fin de frase (~300-500ms de silencio)
- ‚úÖ Procesamiento inmediato al terminar de hablar
- ‚úÖ Barge-in support (interrupci√≥n natural)
- ‚ùå NO click-to-talk buttons
- ‚ùå NO esperar confirmaci√≥n expl√≠cita del usuario

**Quote del usuario**:
> "la conversacion debe estar pensada como si fuera una llamada en tiempo real, con recepcion de audio y se envie cunado temrine d ehabalr el usaurio leugo de ejempl omnedio segundo"

### 2. Interface-Based Architecture (Pluggable Providers)

**Decisi√≥n**: Definir interfaces claras para STT, LLM, y Avatar para permitir implementaciones intercambiables.

**Beneficios**:
1. **Testability**: Mock providers para testing
2. **Fallback**: F√°cil switch entre providers
3. **Gradual migration**: Migrar un componente a la vez
4. **Future-proof**: Nuevos providers sin refactor

**Implementaci√≥n**:
```typescript
interface STTProvider {
  startListening(): Promise<void>;
  onTranscript(callback: (text: string) => void): void;
  // ... m√°s m√©todos
}

// Implementaciones intercambiables:
class DeepgramStreamingSTT implements STTProvider { ... }
class WhisperSTT implements STTProvider { ... }  // Fallback
```

**Ubicaci√≥n**: `lib/realtime-conversation/interfaces.ts`

---

## üåç Decisiones de Lenguaje y Localizaci√≥n

### 3. Deepgram es-419 (LAT-AM Spanish) vs es-CL (Chilean)

**Decisi√≥n**: Usar `es-419` (Latin American Spanish) en lugar de `es-CL` (Chilean Spanish).

**Por qu√© NO es-CL**:
- Deepgram **NO soporta** `es-CL` nativamente
- Opciones disponibles: `es` (general), `es-419` (LAT-AM), `es-ES` (Spain)

**Por qu√© es-419**:
- Es la opci√≥n M√ÅS CERCANA al espa√±ol chileno
- Incluye variaciones de LAT-AM (Argentina, Chile, Colombia, etc.)
- Mejor que `es` gen√©rico o `es-ES` (Espa√±a)

**Trade-off aceptado**:
- Puede tener imperfecciones en modismos muy chilenos ("cachai", "po")
- Mitigaci√≥n: Ajustar confidence threshold si es necesario
- Monitorear accuracy en testing con usuarios chilenos reales

**Configuraci√≥n**:
```typescript
// config/features.ts
deepgram: {
  model: 'nova-2',
  language: 'es-419',  // LAT-AM Spanish (closest to Chilean)
  // ...
}
```

**Referencias**:
- [Deepgram Language Models](https://developers.deepgram.com/docs/models-languages-overview)
- No existe `es-CL` en la lista oficial de Deepgram

**Status**: ‚úÖ Documentado, pendiente de testing real con usuarios chilenos

---

## üß† Decisiones de LLM

### 4. Claude Haiku 4.5 Streaming con AbortController

**Decisi√≥n**: Usar Claude streaming con `AbortController` para interrupciones, no esperar a respuesta completa.

**Por qu√© Streaming**:
- **Latency**: Primera palabra en ~200ms vs ~800ms para respuesta completa
- **UX**: Usuario ve progreso (interim text mientras genera)
- **Barge-in**: Permite interrumpir mid-generation

**Por qu√© AbortController**:
- Claude API no tiene m√©todo `cancel()` nativo
- `AbortController` es el patr√≥n est√°ndar web para cancelar fetch requests
- Compatible con streaming SSE

**Implementaci√≥n**:
```typescript
class ClaudeStreamingLLM implements LLMProvider {
  private abortController: AbortController | null = null;

  async *streamResponse(userMessage: string): AsyncGenerator<string> {
    this.abortController = new AbortController();

    const stream = await anthropic.messages.stream({
      model: 'claude-3-5-haiku-20241022',
      messages: [...this.history, { role: 'user', content: userMessage }],
      max_tokens: 150,
      stream: true,
    }, {
      signal: this.abortController.signal,  // ‚Üê Key: pasar signal
    });

    try {
      for await (const chunk of stream) {
        if (chunk.type === 'content_block_delta') {
          yield chunk.delta.text;
        }
      }
    } catch (error) {
      if (error.name === 'AbortError') {
        console.log('Stream aborted (barge-in)');
      } else {
        throw error;
      }
    }
  }

  interrupt(): void {
    if (this.abortController) {
      this.abortController.abort();  // ‚Üê Cancela el stream
      this.abortController = null;
    }
  }
}
```

**Trade-offs**:
- ‚úÖ Interrupci√≥n inmediata (~10ms)
- ‚úÖ No desperdicia tokens (paga solo lo generado)
- ‚ö†Ô∏è Stream puede terminar mid-sentence (aceptable para barge-in)

**Alternativas consideradas y rechazadas**:
1. ‚ùå **Esperar respuesta completa**: Latency muy alta (~800ms)
2. ‚ùå **Timeout-based cancellation**: No interrumpe el request HTTP
3. ‚ùå **Usar non-streaming API**: No permite barge-in suave

**Referencias**:
- [Claude Streaming API](https://docs.anthropic.com/en/api/messages-streaming)
- [MDN AbortController](https://developer.mozilla.org/en-US/docs/Web/API/AbortController)

**Status**: ‚úÖ Decisi√≥n tomada, implementaci√≥n pendiente (FASE 1.4)

---

## üé≠ Decisiones de Avatar

### 5. HeyGen REPEAT Mode (TTS-only) vs TALK Mode (with LLM)

**Decisi√≥n**: Usar **REPEAT mode** exclusivamente, nunca TALK mode.

**REPEAT Mode**:
- Solo TTS (text-to-speech)
- NO usa LLM interno de HeyGen
- Recibe texto, genera audio + lip-sync

**TALK Mode** (NOT USED):
- Usa GPT-4o mini interno
- Genera respuestas autom√°ticamente
- No podemos controlar el LLM

**Por qu√© REPEAT**:
- ‚úÖ Control total sobre el LLM (usamos Claude)
- ‚úÖ Personalizaci√≥n del prompt (FASE 0 integration)
- ‚úÖ Streaming desde Claude (baja latency)
- ‚úÖ Barge-in support (interrupt Claude, no HeyGen LLM)
- ‚úÖ M√©tricas precisas de LLM latency

**Configuraci√≥n**:
```typescript
// config/features.ts
heygen: {
  taskType: 'REPEAT' as const,  // ALWAYS REPEAT
  taskMode: 'SYNC' as const,    // Wait for previous speech to finish
}

// Usage:
await avatar.speak({
  text: claudeResponse,         // ‚Üê Claude generated this
  task_type: 'REPEAT',          // ‚Üê Never TALK
  task_mode: 'SYNC',
});
```

**Trade-off**:
- ‚ö†Ô∏è Requiere implementar custom pipeline (m√°s complejidad)
- ‚úÖ Pero vale la pena: control total + personalizaci√≥n profunda

**Alternativa rechazada**:
- ‚ùå **TALK mode**: Simple pero pierde personalizaci√≥n y control

**Referencias**:
- [HeyGen Task Types](https://docs.heygen.com/docs/streaming-avatar-sdk#speak)

**Status**: ‚úÖ Decisi√≥n tomada, implementaci√≥n pendiente (FASE 1.5)

---

## ‚è±Ô∏è Decisiones de Latencia

### 6. Target de Latencia: 600-800ms (Realista) vs 500ms (Aspiracional)

**Decisi√≥n**: Target de **600-800ms** total, con componentes bien definidos.

**Breakdown del pipeline**:
```
User stops speaking (T=0ms)
  ‚Üì
Deepgram endpointing delay: 300ms
  ‚Üì
Deepgram processing: 50-100ms
  ‚Üì
Network RTT (user ‚Üí Deepgram ‚Üí server): 50ms
  ‚Üì
Claude first token (TTFT): 200ms
  ‚Üì
Claude full response: +100-200ms (150 tokens @ 50-100 tokens/sec)
  ‚Üì
Network RTT (server ‚Üí HeyGen): 50ms
  ‚Üì
HeyGen TTS processing: 100-150ms
  ‚Üì
HeyGen audio start: 50ms
  ‚Üì
TOTAL: 600-800ms
```

**Por qu√© NO 500ms**:
- Network latency solo (roundtrips): ~150ms unavoidable
- Deepgram endpointing: ~300ms (required for naturalidad)
- Claude TTFT: ~200ms (best case con Haiku)
- HeyGen TTS: ~150ms (no optimizable por nosotros)
- **Total best case**: ~600ms

**Para lograr 500ms se requerir√≠a**:
- ‚ùå Deepgram, Claude, HeyGen en el MISMO VPC/region (no factible)
- ‚ùå Reducir endpointing a 100ms (corta palabras)
- ‚ùå Usar LLM m√°s r√°pido que Haiku (no existe para espa√±ol)

**Target realista con arquitectura actual**:
- ‚úÖ **600ms**: Best case (todo en LAT-AM region, perfect network)
- ‚úÖ **700ms**: Average case (expected)
- ‚úÖ **800ms**: Acceptable case (con network jitter)
- ‚ö†Ô∏è **>1000ms**: Needs optimization o fallback

**Optimizaciones FASE 2** (para acercarse a 600ms):
1. Chunked HeyGen sending (enviar frases conforme se generan)
2. Pre-warm connections (eliminar cold start)
3. Response caching (FAQs comunes)
4. Connection pooling (keep-alive WebSockets)

**User experience**:
- 600ms: "Casi instant√°neo" ‚úÖ
- 800ms: "Natural" ‚úÖ
- 1000ms: "Un poco lento" ‚ö†Ô∏è
- 1500ms: "Necesita mejorar" ‚ùå

**Quote del usuario** (despu√©s de explicar an√°lisis):
> [User approved plan with 600-800ms target]

**Status**: ‚úÖ Decisi√≥n aceptada, m√©tricas en `config/features.ts`

---

## üîÑ Decisiones de State Management

### 7. State Machine con Transiciones Validadas

**Decisi√≥n**: Implementar state machine expl√≠cito con transiciones v√°lidas definidas, no state flags booleanos.

**Por qu√© State Machine**:
- ‚úÖ Previene estados inv√°lidos (ej: `USER_SPEAKING` y `AVATAR_SPEAKING` al mismo tiempo)
- ‚úÖ Transiciones expl√≠citas y auditables
- ‚úÖ Debugging m√°s f√°cil (history de transiciones)
- ‚úÖ Barge-in logic m√°s clara

**Estados definidos**:
```typescript
enum ConversationState {
  IDLE,              // Esperando input
  USER_SPEAKING,     // Usuario hablando
  PROCESSING,        // STT ‚Üí LLM processing
  AVATAR_SPEAKING,   // Avatar hablando
  INTERRUPTED,       // Barge-in detected
  ERROR,             // Error state
}
```

**Transiciones permitidas**:
```typescript
const allowedTransitions = {
  IDLE: [USER_SPEAKING, ERROR],
  USER_SPEAKING: [PROCESSING, IDLE, ERROR],
  PROCESSING: [AVATAR_SPEAKING, IDLE, ERROR],
  AVATAR_SPEAKING: [IDLE, INTERRUPTED, ERROR],  // ‚Üê Barge-in
  INTERRUPTED: [USER_SPEAKING, PROCESSING, IDLE, ERROR],
  ERROR: [IDLE],
};
```

**Barge-in flow**:
```
AVATAR_SPEAKING
  ‚Üì (user starts speaking)
INTERRUPTED
  ‚Üì (avatar stops, user continues)
USER_SPEAKING
  ‚Üì (user finishes)
PROCESSING
  ‚Üì
AVATAR_SPEAKING
```

**Beneficios para debugging**:
```typescript
// Transition history
[
  { from: 'IDLE', to: 'USER_SPEAKING', timestamp: 1234567890, reason: 'speech detected' },
  { from: 'USER_SPEAKING', to: 'PROCESSING', timestamp: 1234567900, reason: 'speech ended' },
  { from: 'PROCESSING', to: 'AVATAR_SPEAKING', timestamp: 1234568000, reason: 'LLM complete' },
  { from: 'AVATAR_SPEAKING', to: 'INTERRUPTED', timestamp: 1234568200, reason: 'barge-in' },
  // ...
]
```

**Alternativa rechazada**:
- ‚ùå **Boolean flags**: `isUserSpeaking`, `isAvatarSpeaking`, `isProcessing`
  - Problema: Estados inconsistentes posibles (`isUserSpeaking && isAvatarSpeaking`)
  - Problema: No hay historial de transiciones

**Ubicaci√≥n**: `lib/realtime-conversation/state-machine.ts`

**Status**: ‚úÖ Implementado en FASE 1.1

---

## üéöÔ∏è Decisiones de Feature Flags

### 8. Progressive Rollout con Feature Flags

**Decisi√≥n**: Todas las features nuevas empiezan **deshabilitadas** y se habilitan manualmente conforme se completan.

**Estructura**:
```typescript
// config/features.ts
export const CONVERSATION_FEATURES = {
  // FASE 1
  ENABLE_STREAMING_STT: process.env.NEXT_PUBLIC_ENABLE_STREAMING_STT === 'true' || false,
  ENABLE_STREAMING_LLM: process.env.NEXT_PUBLIC_ENABLE_STREAMING_LLM === 'true' || false,
  ENABLE_BARGE_IN: process.env.NEXT_PUBLIC_ENABLE_BARGE_IN === 'true' || false,

  // FASE 2
  ENABLE_CHUNKED_HEYGEN: false,
  ENABLE_RESPONSE_CACHE: false,

  // FASE 3
  ENABLE_AUTO_FALLBACK: true,  // ‚Üê Esta s√≠ est√° enabled por defecto
  ENABLE_RETRY_LOGIC: true,

  // Debug
  LOG_LATENCY: process.env.NODE_ENV === 'development',
} as const;
```

**Benefits**:
1. **Safe deployment**: Feature incompleta no afecta production
2. **Easy rollback**: `ENABLE_X=false` sin code changes
3. **Gradual testing**: Habilitar en dev ‚Üí staging ‚Üí production
4. **A/B testing**: Habilitar para % de usuarios

**Workflow**:
```bash
# Development
NEXT_PUBLIC_ENABLE_STREAMING_STT=true npm run dev

# Staging (test en Vercel preview)
# Configure en Vercel: NEXT_PUBLIC_ENABLE_STREAMING_STT=true

# Production (cuando est√© probado)
# Configure en Vercel production: NEXT_PUBLIC_ENABLE_STREAMING_STT=true
```

**Helper**:
```typescript
export function isCustomConversationEnabled(): boolean {
  return (
    CONVERSATION_FEATURES.ENABLE_STREAMING_STT ||
    CONVERSATION_FEATURES.ENABLE_STREAMING_LLM
  );
}

// Usage en component:
if (isCustomConversationEnabled()) {
  // Use custom pipeline
} else {
  // Use HeyGen built-in
}
```

**Status**: ‚úÖ Implementado en `config/features.ts`

---

## üõ°Ô∏è Decisiones de Error Handling

### 9. Fallback Autom√°tico vs Manual

**Decisi√≥n**: Fallback **autom√°tico** despu√©s de 3 fallos consecutivos, con notificaci√≥n al usuario.

**Strategy**:
```typescript
class FallbackManager {
  private failureCount = {
    deepgram: 0,
    claude: 0,
  };

  recordFailure(service: 'deepgram' | 'claude'): void {
    this.failureCount[service]++;

    if (this.failureCount[service] >= 3) {
      this.triggerFallback(service);
    }
  }

  async triggerFallback(service: string): Promise<void> {
    console.warn(`‚ö†Ô∏è Switching to HeyGen built-in (${service} failed 3x)`);

    // Notify user
    showNotification('Cambiando a modo de respaldo para mejor estabilidad...');

    // Switch to HeyGen TALK mode
    await this.switchToHeyGenBuiltIn();

    // Continue conversation (no interruption)
  }
}
```

**Por qu√© autom√°tico**:
- ‚úÖ UX suave: usuario no percibe "app rota"
- ‚úÖ Conversation continues sin restart
- ‚úÖ Self-healing system

**Por qu√© no manual**:
- ‚ùå Requiere user action (mala UX)
- ‚ùå Conversaci√≥n se interrumpe

**Threshold (3 fallos)**:
- 1 fallo: Puede ser network glitch transitorio ‚Üí retry
- 2 fallos: A√∫n puede ser temporal ‚Üí retry
- 3 fallos: Problema real ‚Üí switch a fallback

**User choice confirmado**:
> Q: "Si Deepgram o Claude fallan, ¬øprefieres switch autom√°tico a HeyGen built-in, o esperar/reintentar?"
> A: "Autom√°tico"

**Status**: ‚úÖ Decisi√≥n tomada, implementaci√≥n en FASE 3

---

## üìä Decisiones de Monitoring

### 10. Latency Metrics Collection

**Decisi√≥n**: Capturar m√©tricas detalladas de cada componente del pipeline en development.

**M√©tricas capturadas**:
```typescript
interface LatencyMetrics {
  // Timestamps
  userStopSpeaking: number;
  transcriptReceived: number;
  llmFirstToken: number;
  llmComplete: number;
  avatarStartSpeaking: number;

  // Derived latencies
  sttLatency: number;      // Deepgram
  llmLatency: number;      // Claude
  ttsLatency: number;      // HeyGen
  totalLatency: number;    // End-to-end
}
```

**Por qu√© en development**:
- ‚úÖ Debugging: identificar bottleneck
- ‚úÖ Optimization: medir mejoras de FASE 2
- ‚úÖ No overhead en production (flag: `LOG_LATENCY`)

**Configuraci√≥n**:
```typescript
// config/features.ts
export const CONVERSATION_FEATURES = {
  LOG_LATENCY: process.env.NODE_ENV === 'development' || process.env.NEXT_PUBLIC_LOG_LATENCY === 'true',
  LOG_TRANSCRIPTS: process.env.NODE_ENV === 'development',
  LOG_STATE_TRANSITIONS: process.env.NODE_ENV === 'development',
} as const;
```

**Output example**:
```
üé§ STT latency: 350ms (Deepgram)
üß† LLM latency: 280ms (Claude Haiku)
   ‚Ü≥ First token: 200ms
   ‚Ü≥ Generation: 80ms
üé≠ TTS latency: 180ms (HeyGen)
‚è±Ô∏è Total latency: 810ms
```

**Status**: ‚úÖ Interfaces definidas, implementaci√≥n en FASE 1.6

---

## üóÇÔ∏è Decisiones de Data Flow

### 11. Shopify Fetch: Before Avatar vs After Avatar Start

**Decisi√≥n**: Fetch Shopify data **BEFORE** initializing avatar, no lazy load.

**Flow**:
```
User opens widget
  ‚Üì
Fetch Shopify customer data (if customerId available)
  ‚Üì
Generate personalized prompt with data
  ‚Üì
Initialize avatar with personalized knowledgeBase
  ‚Üì
Avatar greets: "¬°Hola [nombre]! Vi que tienes piel [tipo]..."
```

**Por qu√© before**:
- ‚úÖ Avatar saluda correctamente desde la PRIMERA palabra
- ‚úÖ No hay "generic greeting" seguido de "ah, te reconozco"
- ‚úÖ UX m√°s natural

**Por qu√© no after (lazy)**:
- ‚ùå Avatar dice: "¬°Hola! ¬øC√≥mo te llamo?" (generic)
- ‚ùå Luego: "Ah, eres Mar√≠a!" (awkward)
- ‚ùå Mala UX

**Cache strategy**:
- Fetch once, cache 24 hours en localStorage
- Key: `shopify_customer_${customerId}_${YYYYMMDD}`
- Reduce API calls a Shopify

**User choice confirmado**:
> Q: "¬øObtener datos de Shopify ANTES de iniciar avatar (saludo personalizado from start) o DURANTE conversaci√≥n (lazy load)?"
> A: "Antes, para saludo personalizado desde el inicio"

**Status**: ‚úÖ Implementado en FASE 0

---

## üé§ Decisiones de Audio

### 12. Deepgram Endpointing: 300ms vs 500ms

**Decisi√≥n**: 300ms endpointing delay (tiempo de silencio para detectar fin de frase).

**Trade-offs**:

| Delay | Pro | Contra |
|-------|-----|--------|
| 100ms | Muy r√°pido | Corta palabras (false positive) |
| 300ms | Balance ideal | Ocasionalmente corta frases largas |
| 500ms | No corta nunca | Se siente lento |

**Por qu√© 300ms**:
- ‚úÖ Balance entre naturalidad y velocidad
- ‚úÖ Funciona bien en espa√±ol (pausas naturales)
- ‚úÖ Ajustable v√≠a config si se necesita

**Configuraci√≥n**:
```typescript
// config/features.ts
export const CONVERSATION_TIMING = {
  ENDPOINTING_DELAY_MS: 300,  // Deepgram silence detection
} as const;

// Provider config
deepgram: {
  endpointing: 300,
  // ...
}
```

**Fallback**:
- Si 300ms corta frases en testing ‚Üí aumentar a 400ms
- Si se siente lento ‚Üí reducir a 250ms

**Status**: ‚úÖ Configurado, pendiente de testing real

---

## üîÄ Decisiones de Streaming Strategy

### 13. Full-Response vs Chunked Streaming (HeyGen)

**Decisi√≥n**: FASE 1 usa **full-response** (esperar a Claude completar), FASE 2 agrega **chunked sending**.

**Full-Response (FASE 1)**:
```typescript
// Wait for complete response
const chunks: string[] = [];
for await (const chunk of llm.streamResponse(userInput)) {
  chunks.push(chunk);
}

const fullResponse = chunks.join('');
await avatar.speak(fullResponse, 'REPEAT');  // Send all at once
```

**Pros**:
- ‚úÖ Simple implementation
- ‚úÖ HeyGen genera audio √≥ptimo (full context)
- ‚úÖ Mejor calidad de prosody

**Cons**:
- ‚ö†Ô∏è Latency m√°s alta (espera response completa)

**Chunked Sending (FASE 2)**:
```typescript
// Send sentence-by-sentence
const sentenceBuffer: string[] = [];
for await (const chunk of llm.streamResponse(userInput)) {
  sentenceBuffer.push(chunk);

  // Detect sentence end
  if (chunk.match(/[.!?]\s*$/)) {
    const sentence = sentenceBuffer.join('');
    await avatar.speak(sentence, 'REPEAT');  // Send immediately
    sentenceBuffer = [];
  }
}
```

**Pros**:
- ‚úÖ Latency percibida m√°s baja
- ‚úÖ Avatar empieza a hablar antes

**Cons**:
- ‚ö†Ô∏è M√°s complejo
- ‚ö†Ô∏è Posible prosody sub√≥ptima (frases aisladas)
- ‚ö†Ô∏è Requiere sentence boundary detection

**Decision rationale**:
- FASE 1: Full-response (MVP funcional r√°pido)
- FASE 2: Chunked (optimization cuando FASE 1 funcione)

**Status**: ‚úÖ FASE 1 approach definido, FASE 2 en roadmap

---

## üìù Resumen de Decisiones Clave

| Decisi√≥n | Choice | Rationale | Status |
|----------|--------|-----------|--------|
| Lenguaje STT | es-419 | M√°s cercano a chileno (es-CL no existe) | ‚úÖ Documentado |
| LLM Interrupt | AbortController | √önica forma de cancelar Claude stream | ‚úÖ Documentado |
| Avatar Mode | REPEAT only | Control total del LLM | ‚úÖ Documentado |
| Latency Target | 600-800ms | Realista con arquitectura actual | ‚úÖ Documentado |
| State Management | State Machine | Previene estados inv√°lidos | ‚úÖ Implementado |
| Feature Flags | Disabled by default | Safe rollout | ‚úÖ Implementado |
| Fallback | Autom√°tico (3 fallos) | UX suave | ‚úÖ Documentado |
| Shopify Fetch | Before avatar init | Saludo personalizado | ‚úÖ Implementado |
| Endpointing | 300ms | Balance velocidad/accuracy | ‚úÖ Configurado |
| Streaming | Full-response (F1) | Simple, chunked en F2 | ‚úÖ Documentado |

---

## üîó Referencias

- [01-PLAN.md](./01-PLAN.md) - Ver plan completo de implementaci√≥n
- [04-TECHNOLOGIES.md](./04-TECHNOLOGIES.md) - Deep dive en cada tecnolog√≠a
- [05-CONFIGURATION.md](./05-CONFIGURATION.md) - Configuraci√≥n detallada

---

**√öltima actualizaci√≥n**: 2025-11-22
**Pr√≥xima revisi√≥n**: Despu√©s de implementar FASE 1.3 (validar decisiones con c√≥digo real)
