/**
 * Test script for Claude Haiku Streaming LLM Provider
 *
 * Tests:
 * 1. Streaming response (yields chunks in real-time)
 * 2. Conversation history management
 * 3. Interrupt functionality (AbortController)
 */

import { config } from 'dotenv';
import { resolve } from 'path';
import { ClaudeStreamingLLM } from './lib/realtime-conversation/providers/llm/claude-streaming';

// Load environment variables
config({ path: resolve(process.cwd(), '.env.local') });

async function testClaude() {
  console.log('ğŸ§ª Testing Claude Haiku Streaming LLM\n');

  // Check API key
  const apiKey = process.env.ANTHROPIC_API_KEY;
  if (!apiKey || apiKey === 'your_anthropic_api_key_here') {
    console.error('âŒ Error: ANTHROPIC_API_KEY not found in environment');
    console.error('Please add your Anthropic API key to .env.local');
    console.error('Get one at: https://console.anthropic.com/');
    process.exit(1);
  }

  // Initialize provider
  const llm = new ClaudeStreamingLLM({
    apiKey,
    systemPrompt: `Eres Clara, una asistente experta en skincare de la marca Clara.
Hablas espaÃ±ol chileno de manera natural y amigable.
Ayudas a los clientes con recomendaciones de productos y rutinas de skincare.
Respondes de forma concisa (mÃ¡ximo 2-3 oraciones).`,
  });

  console.log('âœ… Claude initialized\n');

  // Test 1: Streaming response
  console.log('ğŸ“ Test 1: Streaming response');
  console.log('Question: "Hola, Â¿quÃ© crema me recomiendas para piel seca?"');
  console.log('Response: ');

  process.stdout.write('> ');
  for await (const chunk of llm.streamResponse('Hola, Â¿quÃ© crema me recomiendas para piel seca?')) {
    process.stdout.write(chunk);
  }
  console.log('\n');

  // Test 2: Conversation history
  console.log('ğŸ“ Test 2: Conversation history');
  const history = llm.getHistory();
  console.log(`History length: ${history.length} messages`);
  console.log('Last user message:', history[history.length - 2]?.content.substring(0, 50) + '...');
  console.log('Last assistant message:', history[history.length - 1]?.content.substring(0, 50) + '...\n');

  // Test 3: Follow-up question (context maintained)
  console.log('ğŸ“ Test 3: Follow-up question (testing context)');
  console.log('Question: "Â¿Y para quÃ© sirve esa crema?"');
  console.log('Response: ');

  process.stdout.write('> ');
  for await (const chunk of llm.streamResponse('Â¿Y para quÃ© sirve esa crema?')) {
    process.stdout.write(chunk);
  }
  console.log('\n');

  // Test 4: Interrupt (barge-in simulation)
  console.log('ğŸ“ Test 4: Interrupt functionality (simulating barge-in)');
  console.log('Question: "CuÃ©ntame todo sobre tu empresa y todos los productos..."');
  console.log('(Will interrupt after 500ms)\n');

  const generator = llm.streamResponse('CuÃ©ntame todo sobre tu empresa, historia, todos los productos, ingredientes, y todo lo que haces');

  // Start streaming
  const streamPromise = (async () => {
    process.stdout.write('> ');
    try {
      for await (const chunk of generator) {
        process.stdout.write(chunk);
      }
    } catch (error: any) {
      if (error.name !== 'AbortError') {
        console.error('\nâŒ Unexpected error:', error);
      }
    }
  })();

  // Interrupt after 500ms
  setTimeout(() => {
    console.log('\n\nğŸ›‘ Interrupting after 500ms...');
    llm.interrupt();
  }, 500);

  await streamPromise;
  console.log('\nâœ… Interrupt test complete\n');

  // Test 5: Clear history
  console.log('ğŸ“ Test 5: Clear history');
  console.log(`History before clear: ${llm.getHistory().length} messages`);
  llm.clearHistory();
  console.log(`History after clear: ${llm.getHistory().length} messages`);

  console.log('\nâœ… All tests complete!');
}

testClaude().catch(console.error);
